{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n#ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import load_model\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T17:49:17.178309Z","iopub.execute_input":"2021-09-26T17:49:17.178778Z","iopub.status.idle":"2021-09-26T17:49:21.628663Z","shell.execute_reply.started":"2021-09-26T17:49:17.178727Z","shell.execute_reply":"2021-09-26T17:49:21.627846Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install notexbook-theme\n%reload_ext notexbook\n%texify","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will use the pre-trained Keras FaceNet model provided by Hiroki Taniai in this tutorial. It was trained on MS-Celeb-1M dataset and expects input images to be color, to have their pixel values whitened (standardized across all three channels), and to have a square shape of 160×160 pixels.**","metadata":{}},{"cell_type":"markdown","source":"# **1. DETECT FACES**","metadata":{}},{"cell_type":"markdown","source":"**Face detection** is the process of automatically locating faces in a photograph and localizing them by drawing a bounding box around their extent.\n\nwe will use **the Multi-Task Cascaded Convolutional Neural Network**, or MTCNN, for face detection, e.g. finding and extracting faces from photos. This is a state-of-the-art deep learning model for face detection .\n\nMulti-task Cascaded Convolutional Networks (MTCNN) is a framework developed as a solution for both face detection and face alignment.","metadata":{}},{"cell_type":"markdown","source":"**install mtcnn**","metadata":{}},{"cell_type":"code","source":"!pip install mtcnn","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:30.426097Z","iopub.execute_input":"2021-09-26T17:49:30.426328Z","iopub.status.idle":"2021-09-26T17:49:38.322575Z","shell.execute_reply.started":"2021-09-26T17:49:30.426290Z","shell.execute_reply":"2021-09-26T17:49:38.321572Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"confirm that the library was installed correctly by importing the library and printing the version.","metadata":{}},{"cell_type":"code","source":"import mtcnn\nprint(mtcnn.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:38.326748Z","iopub.execute_input":"2021-09-26T17:49:38.327010Z","iopub.status.idle":"2021-09-26T17:49:38.338414Z","shell.execute_reply.started":"2021-09-26T17:49:38.326979Z","shell.execute_reply":"2021-09-26T17:49:38.337415Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The first step is to load an image as a NumPy array, which we can achieve using the PIL library and the open() function. We will also convert the image to RGB, just in case the image has an alpha channel or is black and white.","metadata":{}},{"cell_type":"code","source":"# load image from file\nfrom PIL import Image\nimage = Image.open('../input/5-celebrity-faces-dataset/train/ben_afflek/httpimagesfandangocomrImageRendererredesignstaticimgnoxportraitjpgpcpcpcimagesmasterrepositoryperformerimagespjpg.jpg')\n# convert to RGB, if needed\n\nimage = image.convert('RGB')\n# convert to array\npixels = np.asarray(image)\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:38.339752Z","iopub.execute_input":"2021-09-26T17:49:38.340049Z","iopub.status.idle":"2021-09-26T17:49:38.571469Z","shell.execute_reply.started":"2021-09-26T17:49:38.340001Z","shell.execute_reply":"2021-09-26T17:49:38.570817Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Next, we can create an MTCNN face detector class and use it to detect all faces in the loaded photograph.","metadata":{}},{"cell_type":"code","source":"detector = mtcnn.MTCNN()\nresults = detector.detect_faces(pixels)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:33:47.757231Z","iopub.execute_input":"2021-09-26T18:33:47.757526Z","iopub.status.idle":"2021-09-26T18:33:48.649940Z","shell.execute_reply.started":"2021-09-26T18:33:47.757495Z","shell.execute_reply":"2021-09-26T18:33:48.649198Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"Draw a rectangle arround the face using result retutn","metadata":{}},{"cell_type":"code","source":"x = abs(results[0]['box'][0])\ny = abs(results[0]['box'][1])\nxd = results[0]['box'][2]\nyd = results[0]['box'][3]\nx2 = x + xd\ny2 = y +yd\nfig, ax = plt.subplots()\nrect = patches.Rectangle((x, y), xd, yd, linewidth=1, edgecolor='g', facecolor='none')\nax.add_patch(rect)\nax.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:47.953725Z","iopub.execute_input":"2021-09-26T17:49:47.954037Z","iopub.status.idle":"2021-09-26T17:49:48.147950Z","shell.execute_reply.started":"2021-09-26T17:49:47.953996Z","shell.execute_reply":"2021-09-26T17:49:48.147262Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"use the coordinates to extract the face.","metadata":{}},{"cell_type":"code","source":"face = pixels[y:y2, x:x2]\nimage = Image.fromarray(face)\nimage = image.resize((160, 160))\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:48.149150Z","iopub.execute_input":"2021-09-26T17:49:48.149400Z","iopub.status.idle":"2021-09-26T17:49:48.555139Z","shell.execute_reply.started":"2021-09-26T17:49:48.149371Z","shell.execute_reply":"2021-09-26T17:49:48.554475Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Typing all of this together, the function extract_face() will load a photograph from the loaded filename and return the extracted face. It assumes that the photo contains one face and will return the first face detected.","metadata":{}},{"cell_type":"code","source":"def extract_faces(filename):\n    image = Image.open(filename)\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = np.asarray(image)\n    detector = mtcnn.MTCNN()\n    results = detector.detect_faces(pixels)\n    x = abs(results[0]['box'][0])\n    y = abs(results[0]['box'][1])\n    xd = results[0]['box'][2]\n    yd = results[0]['box'][3]\n    x2 = x + xd\n    y2 = y + yd\n    face = pixels[y:y2, x:x2]\n    image = Image.fromarray(face)\n    image = image.resize((160, 160))\n    face_array = np.asarray(image)\n    return (face_array)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:48.556461Z","iopub.execute_input":"2021-09-26T17:49:48.556885Z","iopub.status.idle":"2021-09-26T17:49:48.564995Z","shell.execute_reply.started":"2021-09-26T17:49:48.556848Z","shell.execute_reply":"2021-09-26T17:49:48.564257Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 2. Face Classification System","metadata":{}},{"cell_type":"markdown","source":"In this section, we will develop a face detection system to predict the identity of a given face.\n\nThe model will be trained and tested using the ‘5 Celebrity Faces Dataset‘ that contains many photographs of five different celebrities.","metadata":{}},{"cell_type":"code","source":"from os import listdir\nfrom os.path import isdir\n\ni = 1\nfolder = '../input/5-celebrity-faces-dataset/train/ben_afflek/'\nfor filename in listdir(folder):\n    path = folder + filename\n    face = extract_faces(path)\n    print(i, face.shape)\n    plt.subplot(7, 7, i)\n    plt.axis('off')\n    plt.imshow(face)\n    i += 1\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:49:48.568187Z","iopub.execute_input":"2021-09-26T17:49:48.568389Z","iopub.status.idle":"2021-09-26T17:50:04.012953Z","shell.execute_reply.started":"2021-09-26T17:49:48.568367Z","shell.execute_reply":"2021-09-26T17:50:04.012240Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Tansform the previous code to a function to load_faces() that take a folder name and return all faces","metadata":{}},{"cell_type":"code","source":"def load_faces(folder):\n    faces = list()\n    for filename in listdir(folder):\n        path = folder + filename\n        face = extract_faces(path)\n        faces.append(face)\n    return (faces)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:50:04.014272Z","iopub.execute_input":"2021-09-26T17:50:04.014534Z","iopub.status.idle":"2021-09-26T17:50:04.020002Z","shell.execute_reply.started":"2021-09-26T17:50:04.014501Z","shell.execute_reply":"2021-09-26T17:50:04.019140Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The load_dataset function below will return a list of images and a label name that we eill take it from folder name. We will implement this function in the train and the  val folder.","metadata":{}},{"cell_type":"code","source":"def load_data(folder):\n    x, y = list(), list()\n    for subdir in listdir(folder):\n        path = folder + subdir +'/'\n        if not isdir(path):\n            continue\n        faces = load_faces(path)\n        labels = [subdir] * len(faces)\n        x.extend(faces)\n        y.extend(labels)\n    return np.asarray(x), np.asarray(y)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:50:04.021575Z","iopub.execute_input":"2021-09-26T17:50:04.021856Z","iopub.status.idle":"2021-09-26T17:50:04.029891Z","shell.execute_reply.started":"2021-09-26T17:50:04.021824Z","shell.execute_reply":"2021-09-26T17:50:04.029019Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Load train Data and test Data (generate a face embeded that the model will use to classificy the new data)","metadata":{}},{"cell_type":"code","source":"x_train, y_train = load_data('../input/5-celebrity-faces-dataset/train/')\nx_test, y_test = load_data('../input/5-celebrity-faces-dataset/val/')\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:50:04.031166Z","iopub.execute_input":"2021-09-26T17:50:04.031417Z","iopub.status.idle":"2021-09-26T17:52:05.832457Z","shell.execute_reply.started":"2021-09-26T17:50:04.031387Z","shell.execute_reply":"2021-09-26T17:52:05.831748Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We will save the data in a zip file to use it next time, without neeed to build it again,","metadata":{}},{"cell_type":"code","source":"np.savez_compressed('5-celebrity-faces.npz', x_train, y_train, x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T17:52:05.833801Z","iopub.execute_input":"2021-09-26T17:52:05.834040Z","iopub.status.idle":"2021-09-26T17:52:06.226723Z","shell.execute_reply.started":"2021-09-26T17:52:05.834007Z","shell.execute_reply":"2021-09-26T17:52:06.226021Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Load the Data from the Zip file","metadata":{}},{"cell_type":"code","source":"data = np.load('5-celebrity-faces.npz')\nx_train, y_train, x_test, y_test = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\nprint('Dataset: train=%d, test=%d' % (x_train.shape[0], x_test.shape[0]))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:19:59.926639Z","iopub.execute_input":"2021-09-26T18:19:59.927206Z","iopub.status.idle":"2021-09-26T18:20:00.000626Z","shell.execute_reply.started":"2021-09-26T18:19:59.927167Z","shell.execute_reply":"2021-09-26T18:19:59.999507Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"Next, we can load our FaceNet model ready for converting faces into face embeddings.","metadata":{}},{"cell_type":"code","source":"# load the model\nmodel = load_model('../input/facenet-keras/facenet_keras.h5')\n# summarize input and output shape\nprint(model.inputs)\nprint(model.outputs)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:34:07.266633Z","iopub.execute_input":"2021-09-26T18:34:07.269095Z","iopub.status.idle":"2021-09-26T18:34:12.355342Z","shell.execute_reply.started":"2021-09-26T18:34:07.269056Z","shell.execute_reply":"2021-09-26T18:34:12.354528Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"To predict an embedding, first the pixel values of the image need to be suitably prepared to meet the expectations of the FaceNet model. This specific implementation of the FaceNet model expects that the pixel values are standardized, and we have to expand the dimension so that the face array is one simple, all that wille be in get_embedding() function","metadata":{}},{"cell_type":"code","source":"def get_embidding(model, face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) / std\n    # transform face into one sample\n    samples = np.expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    prediction = model.predict(samples)\n    return prediction[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:07:14.600696Z","iopub.execute_input":"2021-09-26T18:07:14.601505Z","iopub.status.idle":"2021-09-26T18:07:14.607932Z","shell.execute_reply.started":"2021-09-26T18:07:14.601460Z","shell.execute_reply":"2021-09-26T18:07:14.606884Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Now we eiil convert each face in our data to faceembidding using the above function","metadata":{}},{"cell_type":"code","source":"#transform train_data_x\nx_train_embd = list()\nfor face_pixel in x_train:\n    embd = get_embidding(model, face_pixel)\n    x_train_embd.append(embd)\nx_train_embd = np.asarray(x_train_embd)\n\n#transform test_data_x\nx_test_embd = list()\nfor face_pixel in x_test:\n    embd = get_embidding(model, face_pixel)\n    x_test_embd.append(embd)\nx_test_embd = np.asarray(x_test_embd)\nx_test_embd.shape, x_train_embd.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:34:16.863730Z","iopub.execute_input":"2021-09-26T18:34:16.864294Z","iopub.status.idle":"2021-09-26T18:34:23.728838Z","shell.execute_reply.started":"2021-09-26T18:34:16.864260Z","shell.execute_reply":"2021-09-26T18:34:23.728143Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"## **Perform Face Classification**","metadata":{}},{"cell_type":"markdown","source":"First, it is a good practice to normalize the face embedding vectors. It is a good practice because the vectors are often compared to each other using a distance metric.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer\n# normalize input vectors\nin_encoder = Normalizer(norm='l2')\nx_train_embd = in_encoder.transform(x_train_embd)\nx_test_embd = in_encoder.transform(x_test_embd)\nx_test_embd, x_train_embd","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:20:34.876234Z","iopub.execute_input":"2021-09-26T18:20:34.876503Z","iopub.status.idle":"2021-09-26T18:20:34.889159Z","shell.execute_reply.started":"2021-09-26T18:20:34.876474Z","shell.execute_reply":"2021-09-26T18:20:34.888306Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Next, the string target variables for each celebrity name need to be converted to integers.","metadata":{}},{"cell_type":"code","source":"# label encode targets\nout_encoder = preprocessing.LabelEncoder()\nout_encoder.fit(y_train)\ny_train = out_encoder.transform(y_train)\ny_test = out_encoder.transform(y_test)\ny_test","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:20:53.487967Z","iopub.execute_input":"2021-09-26T18:20:53.488234Z","iopub.status.idle":"2021-09-26T18:20:53.495806Z","shell.execute_reply.started":"2021-09-26T18:20:53.488207Z","shell.execute_reply":"2021-09-26T18:20:53.494817Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"To build a classification MODEL we can use SVC (SUPPORT VECTOR MACHINE) algorithm that is more acuraccy, then we will train our model","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score \n\nmodel = SVC(kernel='linear', probability=True)\n\n#train the project\nmodel.fit(x_test_embd, y_test)\n\ny_predect_train = model.predict(x_train_embd)\ny_predect_test = model.predict(x_test_embd)\n#calcule the ACURRACY\nscore_train = accuracy_score(y_train, y_predect_train)\nscore_test = accuracy_score(y_test, y_predect_test)\nprint(\"accuracy for tarin data : {}\\naccuracy for test data  : {}\".format(score_train * 100, score_test  * 100))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:24:55.252628Z","iopub.execute_input":"2021-09-26T18:24:55.253261Z","iopub.status.idle":"2021-09-26T18:24:55.265123Z","shell.execute_reply.started":"2021-09-26T18:24:55.253222Z","shell.execute_reply":"2021-09-26T18:24:55.264199Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def load_data_2(folder):\n    x, y = list(), list()\n    for filename in listdir(folder):\n        path = folder + filename\n        face = extract_faces(path)\n        labels = [filename]\n        x.append(face)\n        y.extend(labels)\n    return np.asarray(x), np.asarray(y)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:24:58.521140Z","iopub.execute_input":"2021-09-26T18:24:58.521839Z","iopub.status.idle":"2021-09-26T18:24:58.526630Z","shell.execute_reply.started":"2021-09-26T18:24:58.521791Z","shell.execute_reply":"2021-09-26T18:24:58.525936Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"folder = '../input/newimagefortest/'\nnew_faces, new_names = load_data_2(folder)\nmodel_embedd = load_model('../input/facenet-keras/facenet_keras.h5')\ni = 0\nfor face in new_faces:\n    face_embd = get_embidding(model_embedd, np.asarray(face))\n    prob = model.predict_proba([face_embd])\n    out = model.predict([face_embd])\n    out_name = out_encoder.inverse_transform(out)\n    print(\"predict name :  \", out_name[0], \"  with prob :  \", prob[0, out] * 100 , \"   correct name is :       \", new_names[i][:-5])\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2021-09-26T18:32:24.517018Z","iopub.execute_input":"2021-09-26T18:32:24.517715Z","iopub.status.idle":"2021-09-26T18:32:37.498954Z","shell.execute_reply.started":"2021-09-26T18:32:24.517676Z","shell.execute_reply":"2021-09-26T18:32:37.497499Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"We will test our model using new image one image for each label","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}